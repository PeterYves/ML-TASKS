{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o8li1bnhLvU"
      },
      "source": [
        "\n",
        "\n",
        "# Sprint 13 - TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rma2Ctl7hLvZ"
      },
      "source": [
        "### Problem 1 Looking back on scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLVxT0VPhLvZ"
      },
      "source": [
        "From the previous scratches, we:\n",
        "- Had to initialize the weights\n",
        "- Needed an epoch loop\n",
        "- Coded the Activation Functions\n",
        "- Decided the learning rate, sizes, number of nodes and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZDwz8fhLva"
      },
      "source": [
        "### Problem 2 Consider the correspondence between scratch and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D5LcbRfPhLvb",
        "outputId": "7e8f7f2f-fe2e-4537-8d64-2f21210bab60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Epoch 0, loss : 25.1392, val_loss : 58.6584, acc : 0.750, val_acc : 0.375\n",
            "Epoch 1, loss : 19.0486, val_loss : 45.7918, acc : 0.750, val_acc : 0.375\n",
            "Epoch 2, loss : 36.5812, val_loss : 19.9564, acc : 0.250, val_acc : 0.625\n",
            "Epoch 3, loss : 5.6688, val_loss : 16.2655, acc : 0.750, val_acc : 0.375\n",
            "Epoch 4, loss : 4.7483, val_loss : 6.3121, acc : 0.750, val_acc : 0.625\n",
            "Epoch 5, loss : 0.0010, val_loss : 1.8011, acc : 1.000, val_acc : 0.688\n",
            "Epoch 6, loss : 0.0355, val_loss : 1.0384, acc : 1.000, val_acc : 0.875\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.7348, acc : 1.000, val_acc : 0.750\n",
            "Epoch 8, loss : 0.1757, val_loss : 1.5404, acc : 0.750, val_acc : 0.875\n",
            "Epoch 9, loss : 0.3330, val_loss : 4.0418, acc : 0.750, val_acc : 0.500\n",
            "test_acc : 0.550\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wY3X5nRhLve"
      },
      "source": [
        "From the above, we see that the Weights initialization is done through tf.Variable(tf.random_normal); Adam is used as an Optimizer and the activation function passes through tf.nn.relu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZnpIF5ghLvf"
      },
      "source": [
        "### Problem 3 Create a model of Iris using all three types of objective variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "e6GgBbhBhLvg",
        "outputId": "44850ada-4e9f-4caa-c99d-05780ab81324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 99.5450, val_loss : 62.5780, acc : 0.167, val_acc : 0.375\n",
            "Epoch 1, loss : 2.4542, val_loss : 2.2252, acc : 0.667, val_acc : 0.583\n",
            "Epoch 2, loss : 2.4712, val_loss : 1.1667, acc : 0.833, val_acc : 0.833\n",
            "Epoch 3, loss : 0.0000, val_loss : 0.2300, acc : 1.000, val_acc : 0.958\n",
            "Epoch 4, loss : 0.0000, val_loss : 0.2106, acc : 1.000, val_acc : 0.917\n",
            "Epoch 5, loss : 0.0000, val_loss : 0.2939, acc : 1.000, val_acc : 0.958\n",
            "Epoch 6, loss : 0.0000, val_loss : 0.2219, acc : 1.000, val_acc : 0.917\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.3105, acc : 1.000, val_acc : 0.958\n",
            "Epoch 8, loss : 0.0000, val_loss : 0.2476, acc : 1.000, val_acc : 0.917\n",
            "Epoch 9, loss : 0.0000, val_loss : 1.6219, acc : 1.000, val_acc : 0.833\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "\n",
        "y[y=='Iris-setosa'] = 0\n",
        "y[y=='Iris-versicolor'] = 1\n",
        "y[y=='Iris-virginica'] = 2\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    \n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                             \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1TAmgDrhLvi"
      },
      "source": [
        "### Problem 4 Create a model of House Prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lsQYOpWxhLvi",
        "outputId": "923fbcd6-d5b3-4568-bc60-17f25da3ab7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 505179.7500, val_loss : 183147.3750\n",
            "Epoch 1, loss : 72952.0312, val_loss : 64477.9805\n",
            "Epoch 2, loss : 66495.4609, val_loss : 76409.7969\n",
            "Epoch 3, loss : 47725.2188, val_loss : 33650.7773\n",
            "Epoch 4, loss : 29880.3086, val_loss : 32697.1543\n",
            "Epoch 5, loss : 27129.2090, val_loss : 35189.3516\n",
            "Epoch 6, loss : 24775.3242, val_loss : 33868.4102\n",
            "Epoch 7, loss : 25169.3750, val_loss : 36450.7695\n",
            "Epoch 8, loss : 13432.9043, val_loss : 23513.4336\n",
            "Epoch 9, loss : 12182.1709, val_loss : 18963.3008\n",
            "test_mse : 12182.171\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "y_pred = logits\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLo52a5mhLvj"
      },
      "source": [
        "### Problem 5 Create a model of MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P5f5QMMNhLvk",
        "outputId": "859c1c49-24e6-4a3e-c515-cc192ab8c993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 0, loss : 1.1726, val_loss : 1.1916, acc : 0.700, val_acc : 0.706\n",
            "Epoch 1, loss : 1.1710, val_loss : 1.0871, acc : 0.600, val_acc : 0.702\n",
            "Epoch 2, loss : 1.0614, val_loss : 0.5631, acc : 0.600, val_acc : 0.863\n",
            "Epoch 3, loss : 1.1422, val_loss : 0.3883, acc : 0.600, val_acc : 0.893\n",
            "Epoch 4, loss : 0.7619, val_loss : 0.4159, acc : 0.700, val_acc : 0.913\n",
            "Epoch 5, loss : 0.7550, val_loss : 0.2891, acc : 0.600, val_acc : 0.926\n",
            "Epoch 6, loss : 0.7124, val_loss : 0.3159, acc : 0.700, val_acc : 0.924\n",
            "Epoch 7, loss : 0.3614, val_loss : 0.3510, acc : 0.900, val_acc : 0.928\n",
            "Epoch 8, loss : 0.9883, val_loss : 0.3394, acc : 0.600, val_acc : 0.921\n",
            "Epoch 9, loss : 0.2977, val_loss : 0.3318, acc : 0.900, val_acc : 0.929\n",
            "test_acc : 0.929\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xy6sm1-rlQJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "TensorFlow.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}